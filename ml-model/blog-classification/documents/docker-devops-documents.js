
module.exports.getDockerDevOpsDocuments = () => {
    const doc1 = `
        Docker, represented by a logo with a friendly looking whale, is an open source project that facilitates deployment of applications inside of software containers. Its basic functionality is enabled by resource isolation features of the Linux kernel, but it provides a user-friendly API on top of it. The first version was released in 2013, and it has since become extremely popular and is being widely used by many big players such as eBay, Spotify, Baidu, and more. In the last funding round, Docker has landed a huge $95 million.
        Docker is gaining popularity and its usage is spreading like wildfire. The reason for Docker’s growing popularity is the extent to which it can be used in an IT organization. Very few tools out there have the functionality to find itself useful to both developers and as well as system administrators. Docker is one such tool that truly lives up to its promise of Build, Ship and Run.
        Docker is a software containerization platform, meaning you can build your application, package them along with their dependencies into a container and then these containers can be easily shipped to run on other machines.
        Virtualization is the technique of importing a Guest operating system on top of a Host operating system. This technique was a revelation at the beginning because it allowed developers to run multiple operating systems in different virtual machines all running on the same host. This eliminated the need for extra hardware resource. The advantages of Virtual Machines or Virtualization.
        Containerization is the technique of bringing virtualization to the operating system level. While Virtualization brings abstraction to the hardware, Containerization brings abstraction to the operating system. Do note that Containerization is also a type of Virtualization. Containerization is however more efficient because there is no guest OS here and utilizes a host’s operating system, share relevant libraries & resources as and when needed unlike virtual machines. Application specific binaries and libraries of containers run on the host kernel, which makes processing and execution very fast. Even booting-up a container takes only a fraction of a second. Because all the containers share, host operating system and holds only the application related binaries & libraries. They are lightweight and faster than Virtual Machines.
        Docker is a container management service. The keywords of Docker are develop, ship and run anywhere. The whole idea of Docker is for developers to easily develop applications, ship them into containers which can then be deployed anywhere.
        The initial release of Docker was in March 2013 and since then, it has become the buzzword for modern world development, especially in the face of Agile-based projects.
        Docker Hub is a registry service on the cloud that allows you to download Docker images that are built by other communities. You can also upload your own Docker built images to Docker hub. In this chapter, we will see how to download and the use the Jenkins Docker image from Docker hub.
        In Docker, everything is based on Images. An image is a combination of a file system and parameters. Let’s take an example of the following command in Docker.
        Containers are instances of Docker images that can be run using the Docker run command. The basic purpose of Docker is to run containers. Let’s discuss how to work with containers.
        Running of containers is managed with the Docker run command. To run a container in an interactive mode, first launch the Docker container.
        The server is the physical server that is used to host multiple virtual machines.The Host OS is the base machine such as Linux or Windows.
        The Hypervisor is either VMWare or Windows Hyper V that is used to host virtual machines.You would then install multiple operating systems as virtual machines on top of the existing hypervisor as Guest OS.
        `;
    const doc2 = `
        The good thing about the Docker engine is that it is designed to work on various operating systems. We have already seen the installation on Windows and seen all the Docker commands on Linux systems. Now let’s see the various Docker commands on the Windows OS.
        By default, when you launch a container, you will also use a shell command while launching the container as shown below. This is what we have seen in the earlier chapters when we were working with containers.
        Docker also gives you the capability to create your own Docker images, and it can be done with the help of Docker Files. A Docker File is a simple text file with instructions on how to build your images.
        We created our Docker File in the last chapter. It’s now time to build the Docker File. The Docker File can be built with the following command - docker build .
        Public repositories can be used to host Docker images which can be used by everyone else. An example is the images which are available in Docker Hub. Most of the images such as Centos, Ubuntu, and Jenkins are all publicly available for all. We can also make our images available by publishing it to the public repository on Docker Hub.
        For our example, we will use the myimage repository built in the "Building Docker Files" chapter and upload that image to Docker Hub. Let’s first review the images on our Docker host to see what we can push to the Docker registry.
        In Docker, the containers themselves can have applications running on ports. When you run a container, if you want to access the application in the container via a port number, you need to map the port number of the container to the port number of the Docker host. Let’s look at an example of how this can be achieved.
        In our example, we are going to download the Jenkins container from Docker Hub. We are then going to map the Jenkins port number to the port number on the Docker host.
        Registry is the container managed by Docker which can be used to host private repositories.The port number exposed by the container is 5000. Hence with the –p command, we are mapping the same port number to the 5000 port number on our localhost.
        Container Linking allows multiple containers to link with each other. It is a better option than exposing ports. Let’s go step by step and learn how it works.
        Docker has multiple storage drivers that allow one to work with the underlying storage devices. The following table shows the different storage drivers along with the technology used for the storage drivers.
        `;
    const doc3 = `
        Docker takes care of the networking aspects so that the containers can communicate with other containers and also with the Docker Host. If you do an ifconfig on the Docker Host, you will see the Docker Ethernet adapter. This adapter is created when Docker is installed on the Docker Host.
        Node.js is a JavaScript framework that is used for developing server-side applications. It is an open source framework that is developed to run on a variety of operating systems. Since Node.js is a popular framework for development, Docker has also ensured it has support for Node.js applications.
        NGINX is a popular lightweight web application that is used for developing server-side applications. It is an open-source web server that is developed to run on a variety of operating systems. Since nginx is a popular web server for development, Docker has ensured that it has support for nginx.
        The Docker toolbox is developed so that Docker containers can be run on Windows and MacOS. The site for toolbox on Windows is https://docs.docker.com/docker-for-windows/.
        The Docker Cloud is a service provided by Docker in which you can carry out the following operations − Nodes − You can connect the Docker Cloud to your existing cloud providers such as Azure and AWS to spin up containers on these environments..
        Cloud Repository − Provides a place where you can store your own repositories.Continuous Integration − Connect with Github and build a continuous integration pipeline.
        Application Deployment − Deploy and scale infrastructure and containers.Continuous Deployment − Can automate deployments.Docker has logging mechanisms in place which can be used to debug issues as and when they occur. There is logging at the daemon level and at the container level. Let’s look at the different levels of logging.
        Docker Compose is used to run multiple containers as a single service. For example, suppose you had an application which required NGNIX and MySQL, you could create one file which would start both the containers as a service without the need to start each one separately.
        Docker has integrations with many Continuous Integrations tools, which also includes the popular CI tool known as Jenkins. Within Jenkins, you have plugins available which can be used to work with containers. So let’s quickly look at a Docker plugin available for the Jenkins tool.
        Kubernetes is an orchestration framework for Docker containers which helps expose containers as services to the outside world. For example, you can have two services − One service would contain nginx and mongoDB, and another service would contain nginx and redis. Each service can have an IP or service point which can be connected by other applications. Kubernetes is then used to manage these services.
        Next, depending on the version of Ubuntu you have, you will need to add the relevant site to the docker.list for the apt package manager, so that it will be able to detect the Kubernetes packages from the kubernetes site and download them accordingly.
        `;
    const doc4 = `
        DevOps is a collaboration of Development (Dev) and Operations (Ops), it is a term related to enterprise software development. The aim of devops is to enhance the relationship by promoting good communication and better collaboration between these two entities.
        Considering the Devops model, both Dev and Ops teams are no longer isolated. There are some situations where both the units are combined into a single team, the engineers should handle the entire application lifecycle, starting from development and then testing to deployment to operations, and develop a wide range of skills not restricted to a single function.
        In certain DevOps models, Quality Assurance (QA) and Security teams work simultaneously with the devops teams for the entire application lifecycle. When security becomes the primary focus in a devops team then the DevSecOps comes into action.
        The cloud computing cluster enables devops automation with a typical platform for deployment, testing and production. Some enterprise systems having distributed nature in the past weren’t a good fit for the centralized software deployment. Utilizing a cloud platform solves many problems with distributed complexity.
        Automation in devops is becoming cloud-centric. Cloud computing providers such as public and private support devops systemically on their platform, in addition to continuous integration and development tools. This compact integration lessens the cost associated with on-premise devops automation technology, and enables centralised governance. 
        DevOps culture puts impact on small and multidisciplinary teams, who take collective accountability and work autonomously for how primary users experience their software. There is no place like production for a devops team, their aim is to make live experience of the customer better.
        DevOps teams stick to a mindset of growth. They make beliefs clear and in detail, generate better results, and execute hypotheses as experiments. These teams use telemetry and monitoring to collect evidence in production and study results in live environment. 
        In the process of bringing both the development and operations teams together to complete software development, devops culture and set of processes play a crucial role. It enables enterprises to develop and enhance products at a rapid pace than they can with conventional software development processes.
        Due to the arising bugs while programming, the team's experience deployment failures.The smaller development cycles within devops develop more frequent code release. This process becomes easier to identify code defects so the teams can overcome the amount of deployment failures using the principles of agile programming. 
        DevOps enhances the culture of software development, the combined efforts of the teams give rise to increased productivity. This culture aims to work combinely rather than focusing on individual goals. These teams work more effectively and produce better results when the trust between them becomes stronger.
        This helps to enhance the development process and avoids it from further errors to occur. There are different ways to automate tasks in devops. The process of testing code is automated by Continuous integration servers, decreasing the amount of required manual work. Now the software engineers can concentrate on completing tasks which can’t be automated.
        DevOps comprises the custom of continuous delivery, where your code base has been made accessible for testing or production on a daily basis as a fundamental part of its own principles. Companies like Amazon used Continuous delivery to roll out a brand new feature in production at a unbelievable average time of 11.6 seconds between deployments. As we will discuss below, Continuous delivery is a critical factor in supplying the other advantages of DevOps. 
        Continuous delivery implies which features are deployed in production right after they have been developed. Another factor in the enhanced speed of DevOps is the utilization of automated processes for tasks like testing, cloud infrastructure, logging and monitoring.
        Operates the current testing and merging of code, which leads to identify bugs early stages. Other advantages include saving time with fighting merge issues and quick response to the development teams.
        Frequent delivery of software solutions to the environments of production and testing support organizations to fix bugs quickly and respond to constantly changing business requirements.
        `;
    const doc5 = `
        Version control (using GIT), enables teams around the world to interact effectively between regular development activities besides integrating with software development tools for tracking activities like deployments.
        Lean project management and agile planning techniques are used to organise and separate work into sprints, handling team capacity and support teams to swiftly adapt to the changing business needs.
        DevOps is all about practices, principles and developing a collaborative environment that enhances software delivery and improves business value. With a large number of resources available on the web, you can stay updated and can adopt a devops way of thinking.
        Puppet is a configuration management technology to manage the infrastructure on physical or virtual machines. It is an open-source software configuration management tool developed using Ruby which helps in managing complex infrastructure on the fly. This tutorial will help in understanding the building blocks of Puppet and how it works in an infrastructure environment. All the examples and code snippets used in this tutorial are tested. The working code snippets can be simply used in any Puppet setup by changing the current defined names and variables.
        Apache Subversion which is often abbreviated as SVN, is a software versioning and revision control system distributed under an open source license. Subversion was created by CollabNet Inc. in 2000, but now it is developed as a project of the Apache Software Foundation, and as such is part of a rich community of developers and users. This tutorial provides you an understanding on SVN system that is needed to maintain the current and historical versions of files such as source code, web pages, and documentations.
        JIRA is a project management tool used for issues and bugs tracking system. It is widely used as an issue-tracking tool for all types of testing. This tutorial introduces the readers to the fundamental features, usage, and advantages of JIRA. This tutorial will guide the users on how to utilize this tool to track and report bugs in different applications.
        Ansible is simple open source IT engine which automates application deployment, intra service orchestration, cloud provisioning and many other IT tools.
        GIT is a version control system which allows you to track changes in your file and, by using it you can easily coordinate the work among your team.
        Jenkins is a continuous integration server written in Java. You can use it for testing and reporting changes in near real time. Being a developer, it will help you to find and solve bugs in your code rapidly and automate the testing of their build.
        Well, Selenium is a portable software testing framework for web applications. It provides you with an easy interface for developing automated tests.
        An open-source configuration management tool, use to automate the method of inspecting, delivering and operating your software across the entire lifecycle with platform independency.
        Chef is a powerful configuration management automation tool using which you can transform infrastructure into code.Ansible is an open-source tool which provides one of the simplest ways to automate your apps and IT infrastructures such as network configuration, cloud deployments, and creation of development environments.
        Nagios is a powerful monitoring system which enables you and your organization to identify and resolve IT infrastructure problems before they affect critical business processes.
        ELK is a combination of three powerful, opensource tool: Elasticsearch, Logstash and Kibana used to collect insights out of your logs or data.
        Splunk is a software platform to search, analyze and visualize the machine-generated data or logs gathered from the websites, applications, sensors, devices etc. which make up your IT infrastructure and business. 
        `;

    const dockerDevopsDocuments = [
        doc1, doc2, doc3, doc4, doc5
    ];

    return dockerDevopsDocuments;
};
