
module.exports.getNoSQLDocuments = () => {
    const doc1 = `
        NoSQL, as many of you may already know, is basically a database used to manage huge sets of unstructured data, where in the data is not stored in tabular relations like relational databases. Most of the currently existing Relational Databases have failed in solving some of the complex modern problems.
        NoSQL plays a vital role in an enterprise application which needs to access and analyze a massive set of data that is being made available on multiple virtual servers (remote based) in the cloud infrastructure and mainly when the data set is not structured. Hence, the NoSQL database is designed to overcome the Performance, Scalability, Data Modelling and Distribution limitations that are seen in the Relational Databases.
        Unstructured data can be anything like video file, image file, PDF, Emails etc. What does these files have in common, nothing. Structured Information can be extracted from unstructured data, but the process is time consuming. And as more and more modern data is unstructured, there was a need to have something to store such data for growing applications, hence setting path for NoSQL.
        The (classic) Relational Databases follow a vertical architecture where in a single server holds the data, as all the data is related. Relational Databases does not provide Sharding feature by default, to achieve this a lot of efforts has to be put in, because transactional integrity(Inserting/Updating data in transactions), Multiple table JOINS etc cannot be easily achieved in distributed architecture in case of Relational Databases.
        NoSQL Databases have the Sharding feature as default. No additional efforts required. They automatically spread the data across servers, fetch the data in the fastest time from the server which is free, while maintaining the integrity of data.
        MongoDB is a NoSQL database written in C++ language. Some of its drivers use the C programming language as the base. MongoDB is a document oriented database where it stores data in collections instead of tables. The best part of MongoDB is that the drivers are available for almost all the popular programming languages.
        Having seen the good features of MongoDB, now every developer should be able to understand why it is better to use NoSQL based database for big data transactions and for implementing a scalable model. Now, its time to leave behind the schema definitions of RDBMS and get an advantage of using schema-less databases like MongoDB. Let us see some of the vital advantages of MongoDB.
        As explained in the earlier lessons, data in MongoDB is schema-less, which means there is no need of defining a structure for the data before insertion. Since, MongoDB is a document based database, any document within the same collection is not mandatory to have same set of fields or structure. This helps in easily mapping the documents with the entity objects. In general, the documents in a collection of MongoDB will always share the same data structure(recommended for best performance, not mandatory).
        The vital factor or challenge in modelling the data in a given database is load balancing and hence ensuring the performance aspect effectively. It is a mandate while modelling the data to consider the complete usage of data (CRUD operations) along with how the data will be inherited as well.
        It is known that the MongoDB stores the JSON data in a binary encoded format called BSON, where JSON data model actually extends the BSON in order to provide more flexibility with the additional data types and helps in easily encoding and decoding the data across various programming languages. By default, there is no support for the datatypes like date in JSON. But, BSON provides that feature.
        CRUD operations refer to the basic Insert, Read, Update and Delete operations. In the previous chapter, we learnt about how to create a database and drop the database in MongoDB. Now, let us learn how to perform CRUD (Create/Read/Update/Delete) operations in MongoDB.
        Relationships in MongoDB are used to specify how one or more documents are related to each other. In MongoDB, the relationships can be modelled either by Embedded way or by using the Reference approach. These relationships can be of the following forms.
        Sorting the data in any database is one of the vital operations in any database management system. MongoDB provides sort() function in order to sort the data in a collection. Sort function in MongoDB accepts a list of values and an integer value 1 or -1 which states whether the collection to be sorted in ascending (1) or descending (-1) order.
        `;
    const doc2 = `
        Aggregation in MongoDB is nothing but an operation used to process the data that returns the computed results. Aggregation basically groups the data from multiple documents and operates in many ways on those grouped data in order to return one combined result. In sql count(*) and with group by is an equivalent of MongoDB aggregation.
        Aggregate function groups the records in a collection, and can be used to provide total number(sum), average, minimum, maximum etc out of the group selected.
        As per the MongoDB documentation, Map-reduce is a data processing paradigm for condensing large volumes of data into useful aggregated results. MongoDB uses mapReduce command for map-reduce operations. MapReduce is generally used for processing large data sets.
        The map-reduce function first queries the collection, then maps the result documents to emit key-value pairs, which is then reduced based on the keys that have multiple values.
        Starting from version 2.4, MongoDB started supporting text indexes to search inside string content. The Text Search uses stemming techniques to look for specified words in the string fields by dropping stemming stop words like a, an, the, etc. At present, MongoDB supports around 15 languages.
        GridFS is the MongoDB specification for storing and retrieving large files such as images, audio files, video files, etc. It is kind of a file system to store files but its data is stored within MongoDB collections. GridFS has the capability to store files even greater than its document size limit of 16MB.
        GridFS divides a file into chunks and stores each chunk of data in a separate document, each of maximum size 255k.GridFS by default uses two collections fs.files and fs.chunks to store the file's metadata and the chunks.
        Each chunk is identified by its unique _id ObjectId field. The fs.files serves as a parent document. The files_id field in the fs.chunks document links the chunk to its parent.
        Capped collections are fixed-size circular collections that follow the insertion order to support high performance for create, read, and delete operations. By circular, it means that when the fixed size allocated to the collection is exhausted, it will start deleting the oldest document in the collection without providing any explicit commands.
        Capped collections restrict updates to the documents if the update results in increased document size. Since capped collections store documents in the order of the disk storage, it ensures that the document size does not increase the size allocated on the disk. Capped collections are best for storing log information, cache data, or any other high volume data.
        Apache Cassandra is an example of NoSQL Database. It is a distributed, decentralized and an open-source database or a storage system. It is basically used for managing very large amounts of structured data. There is no single point of failure, providing highly available services.
        For a human, the price of a product place an important role. Cassandra, though it is very powerful and reliable, is FREE! It is an open source project by Apache. Because of the open source feature, it gave birth to a huge Cassandra Community, where people discuss their queries and views. Furthermore, there is a possibility of integrating Cassandra with other Apache Open-source projects like Hadoop, Apache Pig, Apache Hive etc. This is the first and an important Cassandra feature.
        Elastic Scalability is one of the biggest advantages. That is, you can easily scale-up or scale-down the cluster in Cassandra. There is a flexibility for adding or deleting any number of nodes from the cluster without disturbances. That is, there is no need of restarting the cluster while scaling up or scaling down. Because of this, Cassandra has a very high throughput for the highest number of nodes. Moreover, there is zero downtime or any pause during scaling. Hence read and write throughput increases simultaneously without delay.
        `;
    const doc3 = `
        It is different from other DBMS. Data Model of Cassandra is basically the way a technology stores data. Moreover, this article will teach us how Cassandra stores data. Also, in this Cassandra data model tutorial, we will see Cassandra Cluster and Cassandra Keyspace.
        So, let’s begin the Cassandra Data Model.In Cassandra Data model, Cassandra database stores data via Cassandra Clusters. Clusters are basically the outermost container of the distributed Cassandra database. The database is distributed over several machines operating together. Every machine acts as a node and has their own replica in case of failures. These nodes are arranged in a ring format as a cluster.
        Cassandra Application Programming Interface (API) is a set of functions and procedures that allow the creation of applications which access the features or data of an operating system, application or, other services.
        The cluster is a collection of nodes that represents a single system. A cluster in Cassandra is one of the shells in the whole Cassandra database. Many Cassandra Clusters combine together to form the database in Cassandra. A Cluster is basically the outermost shell or storage unit in a database. The Cassandra Cluster contains many different layers of storage units. Each layer contains the other.
        Cassandra CRUD Operation stands for Create, Update, Read and Delete or Drop. These operations are used to manipulate data in Cassandra. Apart from this, CRUD operations in Cassandra, a user can also verify the command or the data.
        Cassandra Query Language Shell (CQLSH) is basically a communication medium between Cassandra and the user. CQLSH is a platform that allows the user to launch the Cassandra query language (CQL). The user can perform many operations using cqlsh. Some of them include: defining a schema, inserting and altering data, executing a query etc.. It basically is a coding platform for Cassandra. Hence, a user can program Cassandra to work according to his requirement.
        Data definition command are used for defining the data in the database. They are another type of commands used in cqlsh prompt. In other words, they are used to create data and data storage units in the database. These commands basically are used to create and manipulate keyspace, table, and index.
        There will be many times when a user has to change the data in the tables. These commands are known as Data Manipulation Commands in CQL. These commands are used to manipulate data in a table. The user can use CQL Data Manipulation Commands to change the contents of the table.
        Collection data types in Cassandra are basically a unit in which multiple values are stored. Usually, a single variable defines Cassandra-CQL collection data type. This variable, in turn, contains multiple values. There are a few collection data types, list, set and map. Many functions are performed on these Cassandra collection data types. These functions include create, insert, update and verify.
        Cassandra is the high performance open source distributed database system. It showed the highest growth rate among new database systems and is poised for huge growth. Our course aims to provide you with everything you will need to get started with Cassandra DB. The Course starts with basic Cassandra Concepts and follow it up with CQL and finally teaches you to build a real application using Cassandra and NodeJs. The following sections are covered in the course.
        As technology evolves, the constant upsurge of data creation leads to a continuous need for a more flexible, secure, and reliable manner of storage. Luckily, Cassandra is able to tackle this growing challenge. Cassandra is a flagship NoSQL database with decentralized, fault-tolerant, scalable, and low-cost features making it a core component of cloud computing systems. The more recent versions have greatly improved the security features, making it suitable for use in enterprise systems. When combined with Java and Spring frameworks, Cassandra can formulate a complete application stack thereby enabling efficient data management.
        In Cassandra, by default authentication and authorization options are disabled. You have to configure Cassandra.yaml file for enabling authentication and authorization.
        `;
    const doc4 = `
        HBase is part of the Hadoop ecosystem which offers random real-time read/write access to data in the Hadoop File System.HBase is a Hadoop project which is Open Source, distributed Hadoop database which has its genesis in the Google’sBigtable.
        HBase is a column-oriented database that provides dynamic database schema. Mainly it runs on top of the HDFS and also supports MapReduce jobs. Moreover, for data processing, HBase also supports other high-level languages. There are some special features of Apache HBase, which makes it special, such as, Consistency, High Availability and many more. So, in this article “Best Features of HBase”, let’s learn all these Features of HBase in detail.
        HBase permits Java API in communicating with HBase because HBase is written in Java. However, the fastest way to communicate with HBase is Java API. So, here, we are describing the referenced Java Admin API below, that covers the tasks used for managing tables.
        Basically, to perform CRUD operations on HBase tables we use Java client API for HBase. Since HBase has a Java Native API and it is written in Java thus it offers programmatic access to DML (Data Manipulation Language).
        All the updates in memory as sorted KeyValues are stored in the MemStore. Basically, Data which contains sorted key/values is stored in an HFile. Moreover, per column family, there is one MemStore. Also, all the updates are sorted per column family.
        GraphQL is an open source server-side technology which was developed by Facebook to optimize RESTful API calls. It is an execution engine and a data query language.
        RESTful APIs follow clear and well-structured resource-oriented approach. However, when the data gets more complex, the routes get longer. Sometimes it is not possible to fetch data with a single request. This is where GraphQL comes handy. GraphQL structures data in the form of a graph with its powerful query syntax for traversing, retrieving, and modifying data.
        GraphQL is a specification that describes the behavior of a GraphQL server. It is a set of guidelines on how requests and responses should be handled like supported protocols, format of the data that can be accepted by the server, format of the response returned by the server, etc. The request made by a client to the GraphQL server is called a Query. Another important concept of GraphQL is its transport layer agnostics. It can be used with any available network protocol like TCP, websocket or any other transport layer protocol. It is also neutral to databases, so you can use it with relational or NoSQL databases.
        A GraphQL schema is at the core of any GraphQL server implementation. It describes the functionality available to the client applications that connect to it. We can use any programming language to create a GraphQL schema and build an interface around it.
        The GraphQL runtime defines a generic graph-based schema to publish the capabilities of the data service it represents. Client applications can query the schema within its capabilities. This approach decouples clients from servers and allows both to evolve and scale independently.
        we use Apollo server to execute GraphQL queries. The makeExecutableSchema function in graphql-tools helps you to bind schema and resolvers.
        Resolver is a collection of functions that generate response for a GraphQL query. In simple terms, a resolver acts as a GraphQL query handler. Every resolver function in a GraphQL schema accepts four positional arguments as given below −
        fieldName:(root, args, context, info) => { result }.
        `;
    const doc5 = `
        A GraphQL operation can either be a read or a write operation. A GraphQL query is used to read or fetch values while a mutation is used to write or post values. In either case, the operation is a simple string that a GraphQL server can parse and respond to with data in a specific format. The popular response format that is usually used for mobile and web applications is JSON.
        Mutation queries modify data in the data store and returns a value. It can be used to insert, update, or delete data. Mutations are defined as a part of the schema.
        Queries define what queries you can run on your GraphQL API. By convention, there should be a RootQuery, which contains all the existing queries. I also pointed out the REST equivalent of each query.
        Neo4j is one of the popular Graph Databases and Cypher Query Language (CQL). Neo4j is written in Java Language. This tutorial explains the basics of Neo4j, Java with Neo4j, and Spring DATA with Neo4j. The tutorial is divided into sections such as Neo4j Introduction, Neo4j CQL, Neo4j CQL Functions, Neo4j Admin, etc.
        A graph is a pictorial representation of a set of objects where some pairs of objects are connected by links. It is composed of two elements - nodes (vertices) and relationships (edges).
        Neo4j is a popular Graph Database. Other Graph Databases are Oracle NoSQL Database, OrientDB, HypherGraphDB, GraphBase, InfiniteGraph, and AllegroGraph.
        CQL stands for Cypher Query Language. Like Oracle Database has query language SQL, Neo4j has CQL as query language.In Noe4j, a relationship is an element using which we connect two nodes of a graph. These relationships have direction, type, and the form patterns of data. 
        MERGE command is a combination of CREATE command and MATCH command.Neo4j CQL MERGE command searches for a given pattern in the graph. If it exists, then it returns the results.
        Using the MATCH clause of Neo4j you can retrieve all nodes in the Neo4j database.The OPTIONAL MATCH clause is used to search for the pattern described in it, while using nulls for missing parts of the pattern.
        OPTIONAL MATCH is similar to the match clause, the only difference being it returns null as a result of the missing parts of the pattern.
        Like SQL, Neo4j CQL has provided some aggregation functions to use in RETURN clause. It is similar to GROUP BY clause in SQL.We can use this RETURN + Aggregation Functions in MATCH command to work on a group of nodes and return some aggregated value.
        In real-time applications, we should take backup of our application database regularly, so that we can restore to some working condition at any failure point.
        In Neo4j database, CQL CREATE command always creates a new node or relationship which means even though you use the same values, it inserts a new row. As per our application requirements for some nodes or relationships, we have to avoid this duplication. For this, we should use some database constraints to create a rule on one or more properties of a node or relationship.
        Like SQL, Neo4j database also supports UNIQUE constraint on node or relationship properties. UNIQUE constraint is used to avoid duplicate records and to enforce data integrity rule.
        `;

    const noSQLDocuments = [
        doc1, doc2, doc3, doc4, doc5
    ];

    return noSQLDocuments;
};
