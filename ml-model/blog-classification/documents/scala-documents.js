
module.exports.getScalaDocuments = () => {
    const doc1 = `
        Scala is a portmanteau of ‘scalable’ and ‘language’. It is designed to grow with user demand.A general-purpose programming language, Scala provides support for functional programming and a strong static type system.
        Using certain strings, we can find patterns and lack of patterns in data.’ To convert a string into a regular expression, use the .r method. We import the Regex class from the package scala.util.matching.Regex.
        Scala is a modern multi-paradigm programming language designed to express common programming patterns in a concise, elegant, and type-safe way. Scala has been created by Martin Odersky and he released the first version in 2003. Scala smoothly integrates the features of object-oriented and functional languages. This tutorial explains the basics of Scala in a simple and reader-friendly way.
        Scala Programming is based on Java, so if you are aware of Java syntax, then it's pretty easy to learn Scala. Further if you do not have expertise in Java but if you know any other programming language like C, C++ or Python then it will also help in grasping Scala concepts very quickly.
        Scala has all the same data types as Java, with the same memory footprint and precision.All the data types listed above are objects. There are no primitive types like in Java. This means that you can call methods on an Int, Long, etc.
        Scala access modifiers. Members of packages, classes or objects can be labeled with the access modifiers private and protected, and if we are not using either of these two keywords, then access will be assumed as public. These modifiers restrict accesses to the members to certain regions of code. To use an access modifier, you include its keyword in the definition of members of package, class or object as we will see in the following section.
        Scala has both functions and methods and we use the terms method and function interchangeably with a minor difference. A Scala method is a part of a class which has a name, a signature, optionally some annotations, and some bytecode where as a function in Scala is a complete object which can be assigned to a variable. In other words, a function, which is defined as a member of some object, is called a method.
        A function definition can appear anywhere in a source file and Scala permits nested function definitions, that is, function definitions inside other function definitions. Most important point to note is that Scala function's name can have characters like +, ++, ~, &,-, --, \\, /, :, etc.
        A closure is a function, whose return value depends on the value of one or more variables declared outside this function.In Scala, as in Java, a string is an immutable object, that is, an object that cannot be modified. On the other hand, objects that can be modified, like arrays, are called mutable objects. Strings are very useful objects, in the rest of this section, we present important methods of java.lang.String class.
        Scala provides a data structure, the array, which stores a fixed-size sequential collection of elements of the same type. An array is used to store a collection of data, but it is often more useful to think of an array as a collection of variables of the same type.
        Instead of declaring individual variables, such as number0, number1, ..., and number99, you declare one array variable such as numbers and use numbers[0], numbers[1], and ..., numbers[99] to represent individual variables. This tutorial introduces how to declare array variables, create arrays, and process arrays using indexed variables. The index of the first element of an array is the number zero and the index of the last element is the total number of elements minus one.
        `;
    const doc2 = `
        Scala has a rich set of collection library. Collections are containers of things. Those containers can be sequenced, linear sets of items like List, Tuple, Option, Map, etc. The collections may have an arbitrary number of elements or be bounded to zero or one element (e.g., Option).
        Collections may be strict or lazy. Lazy collections have elements that may not consume memory until they are accessed, like Ranges. Additionally, collections may be mutable (the contents of the reference can change) or immutable (the thing that a reference refers to is never changed). Note that immutable collections may contain mutable items.
        For some problems, mutable collections work better, and for others, immutable collections work better. When in doubt, it is better to start with an immutable collection and change it later if you need mutable ones.
        A trait encapsulates method and field definitions, which can then be reused by mixing them into classes. Unlike class inheritance, in which each class must inherit from just one superclass, a class can mix in any number of traits.
        Traits are used to define object types by specifying the signature of the supported methods. Scala also allows traits to be partially implemented but traits may not have constructor parameters.
        Pattern matching is the second most widely used feature of Scala, after function values and closures. Scala provides great support for pattern matching, in processing the messages.
        A pattern match includes a sequence of alternatives, each starting with the keyword case. Each alternative includes a pattern and one or more expressions, which will be evaluated if the pattern matches. An arrow symbol => separates the pattern from the expressions.
        Scala supports regular expressions through Regex class available in the scala.util.matching package.Scala implicitly converts the String to a RichString and invokes that method to get an instance of Regex. To find a first match of the regular expression, simply call the findFirstIn() method. If instead of finding only the first occurrence we would like to find all occurrences of the matching word, we can use the findAllIn( ) method and in case there are multiple Scala words available in the target string, this will return a collection of all matching words.
        Scala's exceptions work like exceptions in many other languages like Java. Instead of returning a value in the normal way, a method can terminate by throwing an exception. However, Scala doesn't actually have checked exceptions.
        When you want to handle exceptions, you use a try{...}catch{...} block like you would in Java except that the catch block uses matching to identify and handle the exceptions.
        Throwing an exception looks the same as in Java. You create an exception object and then you throw it with the throw keyword as follows.
        Scala allows you to try/catch any exception in a single block and then perform pattern matching against it using case blocks. Try the following example program to handle exception.
        `;
    const doc3 = `
        An extractor in Scala is an object that has a method called unapply as one of its members. The purpose of that unapply method is to match a value and take it apart. Often, the extractor object also defines a dual method apply for building values, but this is not required.
        The unapply method is what turns Test class into an extractor and it reverses the construction process of apply. Where apply takes two strings and forms an email address string out of them, unapply takes an email address and returns potentially two strings: the user and the domain of the address.
        When an instance of a class is followed by parentheses with a list of zero or more parameters, the compiler invokes the apply method on that instance. We can define apply both in objects and in classes.
        Scala is open to make use of any Java objects and java.io.File is one of the objects which can be used in Scala programming to read and write files.
        Methods and values that aren’t associated with individual instances of a class belong in singleton objects, denoted by using the keyword object instead of class.
        Generic classes are classes which take a type as a parameter. They are particularly useful for collection classes.Generic classes take a type as a parameter within square brackets []. One convention is to use the letter A as type parameter identifier, though any parameter name may be used.
        Sometimes it is necessary to express that the type of an object is a subtype of several other types. In Scala this can be expressed with the help of compound types, which are intersections of object types.
        Self-types are a way to declare that a trait must be mixed into another trait, even though it doesn’t directly extend it. That makes the members of the dependency available without imports.
        A self-type is a way to narrow the type of this or another identifier that aliases this. The syntax looks like normal function syntax but means something entirely different.
        `;
    const doc4 = `
        A method with implicit parameters can be applied to arguments just like a normal method. In this case the implicit label has no effect. However, if such a method misses arguments for its implicit parameters, such arguments will be automatically provided.
        Methods in Scala can be parameterized by type as well as value. The syntax is similar to that of generic classes. Type parameters are declared within a pair of brackets while value parameters are enclosed in a pair of parentheses.
        Scala has a built-in type inference mechanism which allows the programmer to omit certain type annotations. It is, for instance, often not necessary in Scala to specify the type of a variable, since the compiler can deduce the type from the initialization expression of the variable. Also return types of methods can often be omitted since they correspond to the type of the body, which gets inferred by the compiler.
        Annotations associate meta-information with definitions. For example, the annotation @deprecated before a method causes the compiler to print a warning if the method is used.
        Higher order function is a function that either takes a function as argument or returns a function. In other words we can say a function which works with function is called higher order function.
        Higher order function allows you to create function composition, lambda function or anonymous function etc.In scala, functions can be composed from other functions. It is a process of composing in which a function represents the application of two composed functions.
        In scala, if you don't specify primary constructor, compiler creates a constructor which is known as primary constructor. All the statements of class body treated as part of constructor. It is also known as default constructor.
        Scala provides a concept of primary constructor with the definition of class. You don't need to define explicitly constructor if your code has only one constructor. It helps to optimize code. You can create primary constructor with zero or more parameters.
        Inheritance is an object oriented concept which is used to reusability of code. You can achieve inheritance by using extends keyword. To achieve inheritance a class must extend to other class. A class which is extended called super or parent class. a class which extends class is called derived or base class.
        In scala, you can override fields also but it has some rules that need to be followed. Below are some examples that illustrate how to override fields.
        `;
    const doc5 = `
        A pretty hot topic lately is machine learning - the inter-sectional discipline closely related to computational statistics that let’s computers learn without being explicitly programmed.
        One important thing about the following text - the aim is to introduce the library, not the concept and theory behind machine learning or statistics in general so an at least basic understanding of these topics is expected from the reader. Also an at least basic knowledge of Spark in general is required.
        This will be based on Apache Spark 2.x API which employs the new DataFrame API as an alternative to the older RDD one. One of the main benefits of the DataFrame approach is that it’s easier to use and more user friendly than the RDD one. Still, the RDD API is still present but put into maintenance mode (it will no longer be extended and will be deprecated when the DataFrame API will reach feature parity with it).
        MLlib (short for Machine Learning Library) is Apache Spark’s machine learning library and provides us with Spark’s superb scalability and ease-of-use when trying to solve machine learning problems. Under the hood MLlib uses Breeze for it’s linear algebra needs.
        The library contains of a pretty extensive set of features that I will now briefly present. A more in-depth description of each feature set will be presented in the later sections.
        As mentioned before, the DataFrame is the new API employed in Spark versions 2.x that is supposed to be a replacement to the older RDD API. A DataFrame is a Spark Dataset (in short - a distributed, strongly-typed collection of data, the interface was introduced in Spark 1.6) organized into named columns (which represent the variables).
        The concept is effectively the same as a table in a relational database or a data frame in R/Python, but with a set of implicit optimizations.
        Familiarity - as mentioned beforehand, the concept is analogous to wider known and used approaches of manipulating data as tables in relational databases or the data frame construct in e.g. R.
        Spark SQL - it enables us accessing and manipulating the data via SQL queries and a SQL-like domain-specific language.Multitude of possible sources - we can construct a DataSet from external databases, existing RDDs, CSV files, JSON and a multitude of other structured data sources.
        As mentioned above - we have multiple possible sources from which we can create a DataFrame. To load a streaming Dataset from an external source we will use the DataStreamReader interface.
        Apache Hive is a data warehouse software package. For interfacing DataFrames with Hive we need a SparkSession with enabled Hive support and all the needed dependencies in the classpath for Spark to load them automatically.
        We can easily interface with any kind of database using JDBC. For it to be possible You need to have the required JDBC driver for the database you want to interface with included in Your classpath.
        We can automatically convert a RDD into a DataFrame. The names of the arguments of the case classes will become the column names. It supports nesting complex types like Seq or Array.
        `;

    const scalaDocuments = [
        doc1, doc2, doc3, doc4, doc5
    ];

    return scalaDocuments;
};
